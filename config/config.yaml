# Global workflow control

# The length of the hash used to identify a parameter combination
hash_length: 7

# Specify the container framework used by each PRM wrapper. Valid options include:
# - docker (default if not specified)
# - singularity -- Also known as apptainer, useful in HPC/HTC environments where docker isn't allowed
# - dsub -- experimental with limited support, used for running on Google Cloud
container_framework: docker

# Only used if container_framework is set to singularity, this will unpack the singularity containers
# to the local filesystem. This is useful when PRM containers need to run inside another container,
# such as would be the case in an HTCondor/OSPool environment.
# NOTE: This unpacks singularity containers to the local filesystem, which will take up space in a way
# that persists after the workflow is complete. To clean up the unpacked containers, the user must
# manually delete them.
unpack_singularity: false

# Allow the user to configure which container registry containers should be pulled from
# Note that this assumes container names are consistent across registries, and that the
# registry being passed doesn't require authentication for pull actions
container_registry:
   base_url: docker.io
   # The owner or project of the registry
   # For example, "reedcompbio" if the image is available as docker.io/reedcompbio/allpairs
   owner: reedcompbio

# This list of algorithms should be generated by a script which checks the filesystem for installs.
# It shouldn't be changed by mere mortals. (alternatively, we could add a path to executable for each algorithm
# in the list to reduce the number of assumptions of the program at the cost of making the config a little more involved)
# Each algorithm has an 'include' parameter. By toggling 'include' to true/false the user can change
# which algorithms are run in a given experiment.
#
# algorithm-specific parameters are embedded in lists so that users can specify multiple. If multiple
# parameters are specified then the algorithm will be run as many times as needed to cover all parameter
# combinations. For instance if we have the following:
# - name: "myAlg"
#   params:
#         include: true
#         a: [1,2]
#         b: [0.5,0.75]
#
# then myAlg will be run on (a=1,b=0.5),(a=1,b=0.75),(a=2,b=0.5), and (a=2,b=0,75). Pretty neat, but be
# careful: too many parameters might make your runs take a long time.

algorithms:
      - name: "pathlinker"
        params:
              include: true
              run1:
                  k: range(100,201,100)

      - name: "omicsintegrator1"
        params:
              include: true
              run1:
                  b: [5, 6]
                  w: np.linspace(0,5,2)
                  d: [10]
                  dummy_mode: ["file"] # Or "terminals", "all", "others"

      - name: "omicsintegrator2"
        params:
              include: true
              run1:
                  b: [4]
                  g: [0]
              run2:
                  b: [2]
                  g: [3]

      - name: "meo"
        params:
              include: true
              run1:
                  max_path_length: [3]
                  local_search: ["Yes"]
                  rand_restarts: [10]

      - name: "mincostflow"
        params:
              include: true
              run1:
                  flow: [1] # The flow must be an int
                  capacity: [1]

      - name: "allpairs"
        params:
              include: true

      - name: "domino"
        params:
              include: true
              run1:
                  slice_threshold: [0.3]
                  module_threshold: [0.05]


# Here we specify which pathways to run and other file location information.
# DataLoader.py can currently only load a single dataset
# Assume that if a dataset label does not change, the lists of associated input files do not change
datasets:
    -
      # Labels can only contain letters, numbers, or underscores
      label: data0
      # To run OmicsIntegrator1 with dummy nodes, add dummy.txt file to node_files
      # or a dummy column to the node table
      node_files: ["node-prizes.txt", "sources.txt", "targets.txt"]
      # DataLoader.py can currently only load a single edge file, which is the primary network
      edge_files: ["network.txt"]
      # Placeholder
      other_files: []
      # Relative path from the spras directory
      data_dir: "input"
    -
      label: data1
      # Reuse some of the same sources file as 'data0' but different network and targets
      node_files: ["node-prizes.txt", "sources.txt", "alternative-targets.txt"]
      edge_files: ["alternative-network.txt"]
      other_files: []
      # Relative path from the spras directory
      data_dir: "input"

gold_standards:
    -
      # Labels can only contain letters, numbers, or underscores
      label: gs0
      node_files: ["gs_nodes0.txt"]
      # edge_files: [] TODO: later iteration
      data_dir: "input"
      # List of dataset labels to compare with the specific gold standard dataset
      dataset_labels: ["data0"]
    -
      label: gs1
      node_files: ["gs_nodes1.txt"]
      data_dir: "input"
      dataset_labels: ["data1", "data0"]

# If we want to reconstruct then we should set run to true.
# TODO: if include is true above but run is false here, algs are not run.
# is this the behavior we want?
reconstruction_settings:

        #set where everything is saved
        locations:

              #place the save path here
              # TODO move to global
              reconstruction_dir: "output"

        run: true

analysis:
      # Create one summary per pathway file and a single summary table for all pathways for each dataset
      summary:
        include: true
      # Create output files for each pathway that can be visualized with GraphSpace
      graphspace:
        include: true
      # Create Cytoscape session file with all pathway graphs for each dataset
      cytoscape:
        include: true
      # Machine learning analysis (e.g. clustering) of the pathway output files for each dataset
      ml:
        # ml analysis per dataset
        include: true
        # adds ml analysis per algorithm output
        # only runs for algorithms with multiple parameter combinations chosen
        aggregate_per_algorithm: true
        # specify how many principal components to calculate
        components: 2
        # boolean to show the labels on the pca graph
        labels: true
        # 'ward', 'complete', 'average', 'single'
        # if linkage: ward, must use metric: euclidean
        linkage: 'ward'
        # 'euclidean', 'manhattan', 'cosine'
        metric: 'euclidean'
      evaluation:
        # evaluation per dataset-goldstandard pair
        # evaluation will not run unless ml include is set to true
        include: true
        # adds evaluation per algorithm per dataset-goldstandard pair
        # evaluation per algortihm will not run unless ml include and ml aggregate_per_algorithm are set to true
        aggregate_per_algorithm: true
