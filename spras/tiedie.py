from pathlib import Path
from typing import Optional

import pandas as pd
from pydantic import BaseModel, ConfigDict

from spras.config.container_schema import ProcessedContainerSettings
from spras.containers import prepare_volume, run_container_and_log
from spras.interactome import (
    convert_directed_to_undirected,
    reinsert_direction_col_undirected,
)
from spras.prm import PRM
from spras.util import add_rank_column, duplicate_edges, raw_pathway_df

__all__ = ["TieDIE", "TieDIEParams"]

class TieDIEParams(BaseModel):
    s: float = 1
    """Network size control factor"""

    d_expr: Optional[str] = None
    """List of significantly differentially expressed genes,
    along with log-FC or FC values (i.e. by edgeR for RNA-Seq or SAM for microarray data.)
    Generated by a sample-dichotomy of interest."""

    a: Optional[float] = None
    """Linker Cutoff (overrides the Size factor)"""

    c: int = 3
    """Search depth for causal paths"""

    p: int = 1000
    """Number of random permutations performed for significance analysis"""

    pagerank: bool = False
    """Use Personalized PageRank to Diffuse """

    all_paths: bool = False
    """Use all paths instead of only causal paths"""

    model_config = ConfigDict(extra='forbid', use_attribute_docstrings=True)

class TieDIE(PRM[TieDIEParams]):
    # we need edges (weighted), source set (with prizes), and target set (with prizes).
    required_inputs = ["edges", "sources", "targets"]
    dois = ["10.1093/bioinformatics/btt471"]

    @staticmethod
    def generate_inputs(data, filename_map):
        """
        Access fields from the dataset and write the required input files
        @param data: dataset
        @param filename_map: a dict mapping file types in the required_inputs to the filename for that type
        - source: input node types with sources (required)
        - target: input node types with targets (required)
        - edges: input edges file (required)
        """
        # ensures the required input are within the filename_map
        for input_type in TieDIE.required_inputs:
            if input_type not in filename_map:
                raise ValueError(f"{input_type} filename is missing")

        # will take the sources and write them to files, and repeats with targets
        for node_type in ["sources", "targets"]:
            nodes = data.get_node_columns([node_type])
            # check if the nodes have prizes or not
            if data.contains_node_columns("prize"):
                node_df = data.get_node_columns(["prize"])
                nodes = pd.merge(nodes, node_df, on="NODEID")
                nodes["sign"] = "+"
                # creates with the node type without headers
                nodes.to_csv(filename_map[node_type],index=False,sep="\t",columns=["NODEID", "prize", "sign"],header=False)
            else:
                # If there aren't prizes but are sources and targets, make prizes based on them
                nodes = data.get_node_columns([node_type])
                # make all nodes have a prize of 1
                nodes["prize"] = 1.0
                nodes["sign"] = "+"
                # creates with the node type without headers
                nodes.to_csv(filename_map[node_type],index=False,sep="\t",columns=["NODEID", "prize", "sign"],header=False)

        # create the network of edges
        edges = data.get_interactome()

        edges = convert_directed_to_undirected(edges)

        edges["type"] = "-a>"
        # drop the weight column
        edges = edges.drop(columns=["Weight"])
        # creates the edges files that contains the head and tail nodes and the weights after them
        edges.to_csv(filename_map["edges"],sep="\t",index=False,columns=["Interactor1", "type", "Interactor2"],header=False)

    # Skips parameter validation step
    @staticmethod
    def run(inputs, output_file, args=None, container_settings=None):
        if not args: args = TieDIEParams()
        if not container_settings: container_settings = ProcessedContainerSettings()

        work_dir = "/spras"

        # Each volume is a tuple (src, dest) - data generated by Docker
        volumes = list()

        bind_path, edges_file = prepare_volume(inputs["edges"], work_dir, container_settings)
        volumes.append(bind_path)

        bind_path, sources_file = prepare_volume(inputs["sources"], work_dir, container_settings)
        volumes.append(bind_path)

        bind_path, targets_file = prepare_volume(inputs["targets"], work_dir, container_settings)
        volumes.append(bind_path)

        out_dir = Path(output_file).parent

        # TieDIE requires that the output directory exist
        out_dir.mkdir(parents=True, exist_ok=True)
        bind_path, mapped_out_dir = prepare_volume(str(out_dir), work_dir, container_settings)
        volumes.append(bind_path) # Use posix path inside the container

        command = [
            "python",
            "/TieDIE/bin/tiedie",
            "--up_heats", sources_file,
            "--down_heats", targets_file,
            "--network", edges_file,
            "--size", str(args.s),
            "--depth", str(args.c),
            "--permute", str(args.p),
            "--pagerank", "True" if args.pagerank else "False",
            "--all_paths", "False" if args.all_paths else "False",
            "--output_folder", mapped_out_dir,
        ]

        print('Running TieDIE with arguments: {}'.format(' '.join(command)), flush=True)

        container_suffix = 'tiedie:v1'
        run_container_and_log('TieDIE',
                             container_suffix,
                             command,
                             volumes,
                             work_dir,
                             out_dir,
                             container_settings)

        # Rename the primary output file to match the desired output filename
        output = Path(out_dir, "tiedie.sif")
        target = Path(output_file)
        output.rename(target)

    @staticmethod
    def parse_output(raw_pathway_file, standardized_pathway_file, params):
        """
        Convert a predicted pathway into the universal format
        @param raw_pathway_file: pathway file produced by an algorithm's run function
        @param standardized_pathway_file: the same pathway written in the universal format
        """
        df = raw_pathway_df(raw_pathway_file, sep='\t', header=None)
        if not df.empty:
            # get rid of the relationship (second) column (since all relationships are the same "-a>")
            df = df.drop(df.columns[1], axis=1)
            df = add_rank_column(df)
            df = reinsert_direction_col_undirected(df)
            df. columns = ['Node1', 'Node2', 'Rank', "Direction"]
            df, has_duplicates = duplicate_edges(df)
            if has_duplicates:
                print(f"Duplicate edges were removed from {raw_pathway_file}")
        df.to_csv(standardized_pathway_file, header=True, index=False, sep='\t')
